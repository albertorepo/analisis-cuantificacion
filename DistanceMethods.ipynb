{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution matching quantification experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to reproduce the results showed in [insert paper name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from quantification.metrics import binary_kl_divergence, absolute_error\n",
    "from quantification.utils.validation import create_bags_with_multiple_prevalence\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DataConversionWarning)\n",
    "warnings.simplefilter(\"ignore\", SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class LinearRegressionWrapper(LogisticRegression):\n",
    "    \"\"\"Class which overrides the sigmoid function of sklearn's LogisticRegression\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1., penalty='l2', dual=False, tol=1e-4, C=1.0,\n",
    "                 fit_intercept=True, intercept_scaling=1, class_weight=None,\n",
    "                 random_state=None, solver='liblinear', max_iter=100,\n",
    "                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):\n",
    "        super(LinearRegressionWrapper, self).__init__(penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state, solver,\n",
    "                         max_iter, multi_class, verbose, warm_start, n_jobs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        prob = self.decision_function(X)\n",
    "        prob = -1 * prob * self.alpha\n",
    "        np.exp(prob, prob)\n",
    "        prob += 1\n",
    "        np.reciprocal(prob, prob)\n",
    "        if prob.ndim == 1:\n",
    "            return np.vstack([1 - prob, prob]).T\n",
    "        else:\n",
    "            # OvR normalization, like LibLinear's predict_probability\n",
    "            prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n",
    "            return prob\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not hasattr(self, \"coef_\"):\n",
    "            raise NotFittedError(\"Call fit before prediction\")\n",
    "        calculate_ovr = self.coef_.shape[0] == 1 or self.multi_class == \"ovr\"\n",
    "        if calculate_ovr:\n",
    "            return self._predict_proba(X)\n",
    "        else:\n",
    "            return softmax(self.decision_function(X), copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_mean(clf, X, y):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y, clf.predict(X), labels=clf.classes_)\n",
    "    fpr = cm[0, 1] / float(cm[0, 1] + cm[0, 0])\n",
    "    tpr = cm[1, 1] / float(cm[1, 1] + cm[1, 0])\n",
    "    return np.sqrt((1 - fpr) * tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 41 datasets.\n"
     ]
    }
   ],
   "source": [
    "datasets_dir = \"datasets\"\n",
    "dataset_files = [file for file in glob.glob(os.path.join(datasets_dir, \"*.csv\")) if file not in [\"datasets/balance.2.csv\", \"datasets/lettersG.csv\", \"datasets/k9.csv\"]]\n",
    "dataset_names = [os.path.split(name)[-1][:-4] for name in dataset_files]\n",
    "print(\"There are a total of {} datasets.\".format(len(dataset_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dataset_names).index(\"coil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets = len(dataset_names)\n",
    "\n",
    "columns=['dataset', 'method', 'truth', 'predictions', 'kld', 'mae']\n",
    "errors_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "num_bags = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dfile):\n",
    "    df = pd.read_csv(dfile, header=None)\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values.astype(np.int)\n",
    "    if -1 in np.unique(y):\n",
    "        y[y == -1] = 0\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test = normalize(X_train, X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_grid = { \n",
    "           \"n_estimators\" : [9, 18, 27, 36, 45, 54, 63],\n",
    "           \"max_depth\" : [1, 5, 10, 15, 20, 25, 30],\n",
    "           \"min_samples_leaf\" : [1, 2, 4, 6, 8, 10]}\n",
    "#estimator_grid = {'C': [10 ** i for i in range(-3, 3)]}\n",
    "grid_params = dict(verbose=False, scoring=g_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>truth</th>\n",
       "      <th>predictions</th>\n",
       "      <th>kld</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EDX</th>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDy</th>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDX</th>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDy</th>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "      <td>41000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  truth  predictions    kld    mae\n",
       "method                                           \n",
       "EDX       41000  41000        41000  41000  41000\n",
       "EDy       41000  41000        41000  41000  41000\n",
       "HDX       41000  41000        41000  41000  41000\n",
       "HDy       41000  41000        41000  41000  41000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df = pd.read_csv(\"results_nonlinear_geometric_clf_1000bags.csv\")\n",
    "errors_df.groupby(\"method\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantification.dm import HDX\n",
    "\n",
    "hdx = HDX(b=8)\n",
    "\n",
    "for dname, dfile in tqdm(zip(dataset_names, dataset_files), total=n_datasets):\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = load_data(dfile)\n",
    "    \n",
    "    hdx.fit(X_train, y_train)\n",
    "    for X_test_, y_test_, prev_true, in create_bags_with_multiple_prevalence(X_test, y_test, num_bags):\n",
    "        prev_true = prev_true[1]\n",
    "        prev_pred = hdx.predict(X_test_)[1]\n",
    "\n",
    "        kld = binary_kl_divergence(prev_true, prev_pred)\n",
    "        mae = absolute_error(prev_true, prev_pred)\n",
    "\n",
    "        errors_df = errors_df.append(pd.DataFrame([[dname, 'HDX', prev_true, prev_pred, kld, mae]], columns=columns))\n",
    "        \n",
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantification.dm import HDy\n",
    "\n",
    "hdy = HDy(b=8, estimator_class=RandomForestClassifier(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "#hdy = HDy(b=8, estimator_class=LogisticRegression(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "\n",
    "\n",
    "for dname, dfile in tqdm(zip(dataset_names, dataset_files), total=n_datasets):\n",
    "            \n",
    "    X_train, X_test, y_train, y_test = load_data(dfile)\n",
    "    \n",
    "    hdy.fit(X_train, y_train)\n",
    "    for X_test_, y_test_, prev_true, in create_bags_with_multiple_prevalence(X_test, y_test, num_bags):\n",
    "        prev_true = prev_true[1]\n",
    "        prev_pred = hdy.predict(X_test_)[1]\n",
    "\n",
    "        kld = binary_kl_divergence(prev_true, prev_pred)\n",
    "        mae = absolute_error(prev_true, prev_pred)\n",
    "    \n",
    "        errors_df = errors_df.append(pd.DataFrame([[dname, 'HDy', prev_true, prev_pred, kld, mae]], columns=columns))\n",
    "        \n",
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantification.dm import EDx\n",
    "\n",
    "edx = EDx()\n",
    "\n",
    "for dname, dfile in tqdm(zip(dataset_names, dataset_files), total=n_datasets):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = load_data(dfile)\n",
    "    \n",
    "    edx.fit(X_train, y_train)\n",
    "    for X_test_, y_test_, prev_true, in create_bags_with_multiple_prevalence(X_test, y_test, num_bags):\n",
    "        prev_true = prev_true[1]\n",
    "        try:\n",
    "            prev_pred = edx.predict(X_test_)[1]\n",
    "        except ValueError:\n",
    "            errors_df = errors_df.append(pd.DataFrame([[dname, 'EDX', prev_true, np.nan, np.nan, np.nan]], columns=columns))\n",
    "            continue\n",
    "        kld = binary_kl_divergence(prev_true, prev_pred)  \n",
    "        mae = absolute_error(prev_true, prev_pred)\n",
    "\n",
    "        errors_df = errors_df.append(pd.DataFrame([[dname, 'EDX', prev_true, prev_pred, kld, mae]], columns=columns))\n",
    "\n",
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantification.dm import EDy\n",
    "\n",
    "edy = EDy(estimator_class=RandomForestClassifier(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "#edy = EDy(estimator_class=LogisticRegression(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "for dname, dfile in tqdm(zip(dataset_names, dataset_files), total=n_datasets):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = load_data(dfile)\n",
    "    \n",
    "    edy.fit(X_train, y_train)\n",
    "    for X_test_, y_test_, prev_true, in create_bags_with_multiple_prevalence(X_test, y_test, num_bags):\n",
    "        prev_true = prev_true[1]\n",
    "        prev_pred = edy.predict(X_test_)[1]\n",
    "\n",
    "        kld = binary_kl_divergence(prev_true, prev_pred)\n",
    "        mae = absolute_error(prev_true, prev_pred)\n",
    " \n",
    "        errors_df = errors_df.append(pd.DataFrame([[dname, 'EDy', prev_true, prev_pred, kld, mae]], columns=columns))\n",
    "\n",
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [30:06<00:00, 44.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from quantification.cc import AC\n",
    "\n",
    "ac = AC(estimator_class=RandomForestClassifier(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "#ac = AC(estimator_class=LogisticRegression(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "for dname, dfile in tqdm(zip(dataset_names, dataset_files), total=n_datasets):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = load_data(dfile)\n",
    "    \n",
    "    ac.fit(X_train, y_train)\n",
    "    for X_test_, y_test_, prev_true, in create_bags_with_multiple_prevalence(X_test, y_test, num_bags):\n",
    "        prev_true = prev_true[1]\n",
    "        prev_pred = ac.predict(X_test_)[1]\n",
    "\n",
    "        kld = binary_kl_divergence(prev_true, prev_pred)\n",
    "        mae = absolute_error(prev_true, prev_pred)\n",
    " \n",
    "        errors_df = errors_df.append(pd.DataFrame([[dname, 'AC', prev_true, prev_pred, kld, mae]], columns=columns))\n",
    "    \n",
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CvMy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 34/41 [26:13<05:23, 46.27s/it]"
     ]
    }
   ],
   "source": [
    "from quantification.dm import CvMy\n",
    "\n",
    "# cvmy = CvMy(estimator_class=RandomForestClassifier(random_state=random_state, class_weight='balanced'))\n",
    "cvmy = CvMy(estimator_class=RandomForestClassifier(random_state=random_state, class_weight='balanced'), estimator_grid=estimator_grid, grid_params=grid_params)\n",
    "for dname, dfile in tqdm(zip(dataset_names, dataset_files), total=n_datasets):\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = load_data(dfile)\n",
    "    \n",
    "    cvmy.fit(X_train, y_train)\n",
    "    for X_test_, y_test_, prev_true, in create_bags_with_multiple_prevalence(X_test, y_test, num_bags):\n",
    "        prev_true = prev_true[1]\n",
    "        prev_pred = cvmy.predict(X_test_)[1]\n",
    "\n",
    "        kld = binary_kl_divergence(prev_true, prev_pred)\n",
    "        mae = absolute_error(prev_true, prev_pred)\n",
    " \n",
    "        errors_df = errors_df.append(pd.DataFrame([[dname, 'CvMy', prev_true, prev_pred, kld, mae]], columns=columns))\n",
    "    \n",
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">kld</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AC</th>\n",
       "      <td>0.15923</td>\n",
       "      <td>0.00344</td>\n",
       "      <td>0.05677</td>\n",
       "      <td>0.03093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CvMy</th>\n",
       "      <td>0.24658</td>\n",
       "      <td>0.00590</td>\n",
       "      <td>0.06392</td>\n",
       "      <td>0.04053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDX</th>\n",
       "      <td>0.03458</td>\n",
       "      <td>0.00391</td>\n",
       "      <td>0.05157</td>\n",
       "      <td>0.03349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDy</th>\n",
       "      <td>0.15613</td>\n",
       "      <td>0.00361</td>\n",
       "      <td>0.05323</td>\n",
       "      <td>0.03187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDX</th>\n",
       "      <td>0.03368</td>\n",
       "      <td>0.00453</td>\n",
       "      <td>0.06157</td>\n",
       "      <td>0.03603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDy</th>\n",
       "      <td>0.04134</td>\n",
       "      <td>0.00283</td>\n",
       "      <td>0.05450</td>\n",
       "      <td>0.02786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           kld             mae        \n",
       "          mean  median    mean  median\n",
       "method                                \n",
       "AC     0.15923 0.00344 0.05677 0.03093\n",
       "CvMy   0.24658 0.00590 0.06392 0.04053\n",
       "EDX    0.03458 0.00391 0.05157 0.03349\n",
       "EDy    0.15613 0.00361 0.05323 0.03187\n",
       "HDX    0.03368 0.00453 0.06157 0.03603\n",
       "HDy    0.04134 0.00283 0.05450 0.02786"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df.groupby(['method'])[['kld', 'mae']].agg(['mean', 'median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors_df.kld[errors_df.kld > .1] = np.nan\n",
    "# errors_df.mae[errors_df.mae > .1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(errors_df, row=\"method\", row_order=errors_df.method.unique(),\n",
    "                  size=1.7, aspect=4,)\n",
    "\n",
    "_ = g.map(sns.kdeplot, \"kld\",  shade=True)\n",
    "_ = g.map(sns.rugplot, \"kld\")\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "_ = g.fig.suptitle('KLD') # can also get the figure from plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(errors_df, row=\"method\", row_order=errors_df.method.unique(),\n",
    "                  size=1.7, aspect=4,)\n",
    "\n",
    "_ = g.map(sns.kdeplot, \"mae\",  shade=True)\n",
    "_ = g.map(sns.rugplot, \"mae\")\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "_ = g.fig.suptitle('MAE') # can also get the figure from plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "output_notebook()\n",
    "\n",
    "palette = sns.palettes.color_palette('colorblind', n_datasets)\n",
    "colors = itertools.cycle(palette.as_hex())\n",
    "p = figure(plot_width=800, plot_height=1400)\n",
    "\n",
    "for dname, color in zip(dataset_names, colors):\n",
    "    df = errors_df[np.logical_and(errors_df.dataset==dname, errors_df.method==\"HDy\")]\n",
    "    p.scatter(df['truth'], df['mae'], color=color, legend=dname)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.groupby([\"dataset\"])[\"kld\", \"mae\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = errors_df.pivot_table(index=['dataset'], \n",
    "                    columns='method', \n",
    "                    aggfunc=np.mean, \n",
    "                    values='mae', \n",
    "                    fill_value=0)\n",
    "mae_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.to_csv(\"results_nonlinear_geometric_clf_1000bags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df.method.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3db1c83b11e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maxs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maxs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maxs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'figure'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2, axs2 = plt.subplots(2,2)\n",
    "axs2.figure\n",
    "axs2 = np.array([axs[0][1], axs[2][3], axs[1][4], axs[3][4]])\n",
    "axs2.figure = fig2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
